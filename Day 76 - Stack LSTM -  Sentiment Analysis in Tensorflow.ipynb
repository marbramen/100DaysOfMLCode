{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using cuDNN version 6021 on context None\n",
      "Mapped name None to device cuda: GeForce GTX 1050 (0000:01:00.0)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "maxlen = 250\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "input_train shape: (25000, 250)\n",
      "input_test shape: (25000, 250)\n",
      "y_train (25000,)\n",
      "y_test (25000,)\n"
     ]
    }
   ],
   "source": [
    "print(len(input_train), 'train sequences')\n",
    "print(len(input_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "input_train = sequence.pad_sequences(input_train, maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(input_test, maxlen=maxlen)\n",
    "\n",
    "print('input_train shape:', input_train.shape)\n",
    "print('input_test shape:', input_test.shape)\n",
    "print('y_train', y_train.shape)\n",
    "print('y_test', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_FROM = 3\n",
    "word_to_id = imdb.get_word_index()\n",
    "word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN \n",
      "Integers:\n",
      "----------\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    1    4  204 7610   20   16   93   11 9075   19    2\n",
      " 4390    6   55   52   22  849 4227  119    7 5259  961  178    6 1018\n",
      "  221   20 1184    2    2   29    7  265   16  530   17   29  220  210\n",
      "  468    8   30   11   32    7   27  102 5910 3634   17 3278 1881   16\n",
      "    6 6647    7 1262  190    4   20  122 2353    8   79    6  117  196\n",
      "   11 1370   12  127   24  847   33    4 1062    7    4 9075  310  131\n",
      "   12    9    6  253   20   15  144   30  110   33  222  280]\n",
      "\n",
      "\n",
      "Sentences:\n",
      "----------\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <START> the original demille movie was made in 1938 with <UNK> march a very good film indeed hollywood's love of remakes brings us a fairly interesting movie starring <UNK> <UNK> he of course was brilliant as he almost always seemed to be in all of his movies charlton heston as andrew jackson was a stroke of genius however the movie did tend to get a little long in places it does not move at the pace of the 1938 version still it is a fun movie that should be seen at least once\n",
      "\n",
      "\n",
      "TEST \n",
      "Sentence train : 24  value 1\n"
     ]
    }
   ],
   "source": [
    "id_example = 24\n",
    "print(\"TRAIN \")\n",
    "print(\"Integers:\")\n",
    "print(\"-\"*10)\n",
    "print(input_train[id_example])\n",
    "print(\"\\n\")\n",
    "print(\"Sentences:\")\n",
    "print(\"-\"*10)\n",
    "print(' '.join(id_to_word[id] for id in input_train[id_example]))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"TEST \")\n",
    "print(\"Sentence train :\", id_example, \" value\", y_train[id_example])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN \n",
      "Integers:\n",
      "----------\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    1  194 1153  194 8255   78  228    5    6\n",
      " 1463 4369 5012  134   26    4  715    8  118 1634   14  394   20   13\n",
      "  119  954  189  102    5  207  110 3103   21   14   69  188    8   30\n",
      "   23    7    4  249  126   93    4  114    9 2300 1523    5  647    4\n",
      "  116    9   35 8163    4  229    9  340 1322    4  118    9    4  130\n",
      " 4901   19    4 1002    5   89   29  952   46   37    4  455    9   45\n",
      "   43   38 1543 1905  398    4 1649   26 6853    5  163   11 3215    2\n",
      "    4 1153    9  194  775    7 8255    2  349 2637  148  605    2 8003\n",
      "   15  123  125   68    2 6853   15  349  165 4362   98    5    4  228\n",
      "    9   43    2 1157   15  299  120    5  120  174   11  220  175  136\n",
      "   50    9 4373  228 8255    5    2  656  245 2350    5    4 9837  131\n",
      "  152  491   18    2   32 7464 1212   14    9    6  371   78   22  625\n",
      "   64 1382    9    8  168  145   23    4 1690   15   16    4 1355    5\n",
      "   28    6   52  154  462   33   89   78  285   16  145   95]\n",
      "\n",
      "\n",
      "Sentences:\n",
      "----------\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <START> big hair big boobs bad music and a giant safety pin these are the words to best describe this terrible movie i love cheesy horror movies and i've seen hundreds but this had got to be on of the worst ever made the plot is paper thin and ridiculous the acting is an abomination the script is completely laughable the best is the end showdown with the cop and how he worked out who the killer is it's just so damn terribly written the clothes are sickening and funny in equal <UNK> the hair is big lots of boobs <UNK> men wear those cut <UNK> shirts that show off their <UNK> sickening that men actually wore them and the music is just <UNK> trash that plays over and over again in almost every scene there is trashy music boobs and <UNK> taking away bodies and the gym still doesn't close for <UNK> all joking aside this is a truly bad film whose only charm is to look back on the disaster that was the 80's and have a good old laugh at how bad everything was back then\n",
      "\n",
      "\n",
      "TEST \n",
      "Sentence train : 1  value 0\n"
     ]
    }
   ],
   "source": [
    "id_example = 1\n",
    "print(\"TRAIN \")\n",
    "print(\"Integers:\")\n",
    "print(\"-\"*10)\n",
    "print(input_train[id_example])\n",
    "print(\"\\n\")\n",
    "print(\"Sentences:\")\n",
    "print(\"-\"*10)\n",
    "print(' '.join(id_to_word[id] for id in input_train[id_example]))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"TEST \")\n",
    "print(\"Sentence train :\", id_example, \" value\", y_train[id_example])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train))\n",
    "print(y_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n",
      "(25000, 2)\n"
     ]
    }
   ],
   "source": [
    "#Convert y_train, y_test a multiclass\n",
    "def convert_to_multiclass(data):\n",
    "    y = []\n",
    "    for i in range(len(data)):\n",
    "        if data[i] == 1:\n",
    "            y.append([1,0])\n",
    "        else:\n",
    "            y.append([0,1])\n",
    "    y = np.array(y)\n",
    "    return y\n",
    "\n",
    "y_train_mc = convert_to_multiclass(y_train)\n",
    "y_test_mc = convert_to_multiclass(y_test)\n",
    "            \n",
    "print(y_train_mc.shape)\n",
    "print(y_test_mc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 250)\n",
      "(32, 2)\n"
     ]
    }
   ],
   "source": [
    "def next_batch(batch_size, data, labels):\n",
    "    idx = np.arange(0, len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:batch_size]\n",
    "    data_shuffle = [data[i] for i in idx]\n",
    "    labels_shuffle = [labels[i] for i in idx]\n",
    "    \n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n",
    "\n",
    "input_train_batch, y_train_batch = next_batch(32, input_train, y_train_mc)\n",
    "print(input_train_batch.shape)\n",
    "print(y_train_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 32\n",
    "lstmUnits = 64\n",
    "numClasses = 2\n",
    "iterations = 1500\n",
    "num_layers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholders for input and labels\n",
    "\n",
    "tf.reset_default_graph()\n",
    "labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, maxlen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 250, 300)\n"
     ]
    }
   ],
   "source": [
    "# embedding layer\n",
    "num_embeddings = 300\n",
    "\n",
    "embedding_matrix = tf.Variable(tf.random_uniform([max_features, num_embeddings], \n",
    "                                                 -1.0, \n",
    "                                                 1.0))\n",
    "\n",
    "embedding_layer = tf.nn.embedding_lookup(embedding_matrix, input_data)\n",
    "print(embedding_layer.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_rnn_LSTMcell(lstm_size):\n",
    "    cell = tf.contrib.rnn.LSTMCell(lstm_size, state_is_tuple=True)\n",
    "    cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob = 0.75)\n",
    "    print(cell.state_size)\n",
    "    return cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMStateTuple(c=64, h=64)\n",
      "LSTMStateTuple(c=64, h=64)\n",
      "LSTMStateTuple(c=64, h=64)\n",
      "(LSTMStateTuple(c=64, h=64), LSTMStateTuple(c=64, h=64), LSTMStateTuple(c=64, h=64))\n",
      "(LSTMStateTuple(c=<tf.Tensor 'strided_slice:0' shape=(32, 64) dtype=float32>, h=<tf.Tensor 'strided_slice_1:0' shape=(32, 64) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'strided_slice_2:0' shape=(32, 64) dtype=float32>, h=<tf.Tensor 'strided_slice_3:0' shape=(32, 64) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'strided_slice_4:0' shape=(32, 64) dtype=float32>, h=<tf.Tensor 'strided_slice_5:0' shape=(32, 64) dtype=float32>))\n"
     ]
    }
   ],
   "source": [
    "# LSTM layer and dropout layer\n",
    "lstmCell = tf.contrib.rnn.MultiRNNCell(cells=[make_rnn_LSTMcell(lstmUnits) for _ in range(num_layers)], \n",
    "                                       state_is_tuple=True)\n",
    "print(lstmCell.state_size)\n",
    "\n",
    "'''\n",
    "state_placeholder = tf.placeholder(tf.float32, [num_layers, 2, batchSize, lstmUnits])\n",
    "l_unstack = tf.unstack(state_placeholder, axis=0)\n",
    "rnn_tuple_state = tuple([tf.nn.rnn_cell.LSTMStateTuple(\n",
    "    l_unstack[idx][0], l_unstack[idx][1])\n",
    "    for idx in range(num_layers)])\n",
    "print(rnn_tuple_state)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_state = lstmCell.zero_state(batchSize, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_lstm, _ = tf.nn.dynamic_rnn(lstmCell, \n",
    "                                   embedding_layer,\n",
    "                                   #initial_state = rnn_tuple_state,\n",
    "                                   initial_state = zero_state,\n",
    "                                   dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "value = tf.transpose(encode_lstm, [1,0,2])\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "prediction = (tf.matmul(last, weight) + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics for correct prediction and accuracy\n",
    "correctPred = tf.equal(tf.argmax(prediction, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard cross entropy loss with softmax layer\n",
    "# adam optmizer\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, \n",
    "                                                              labels = labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"/home/marchelo/MarcheloBragagnini/100DaysOfMLCode/tensorboard\" + \"/\" + datetime.datetime.now().strftime(\"%Y%m%s-%H%M%S\") + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: loss 0.764886 , acc: 0.406250\n",
      "Iteration 50: loss 0.638540 , acc: 0.718750\n",
      "Iteration 100: loss 0.567478 , acc: 0.750000\n",
      "Iteration 150: loss 0.519135 , acc: 0.812500\n",
      "Iteration 200: loss 0.547022 , acc: 0.687500\n",
      "Iteration 250: loss 0.537737 , acc: 0.781250\n",
      "Iteration 300: loss 0.608097 , acc: 0.625000\n",
      "Iteration 350: loss 0.271229 , acc: 0.906250\n",
      "Iteration 400: loss 0.371059 , acc: 0.781250\n",
      "Iteration 450: loss 0.573897 , acc: 0.812500\n",
      "Iteration 500: loss 0.287371 , acc: 0.875000\n",
      "saved to ./weights_models/pretrained_lstm_SA.ckpt-500\n",
      "Iteration 550: loss 0.494807 , acc: 0.781250\n",
      "Iteration 600: loss 0.305760 , acc: 0.875000\n",
      "Iteration 650: loss 0.167991 , acc: 0.937500\n",
      "Iteration 700: loss 0.247390 , acc: 0.937500\n",
      "Iteration 750: loss 0.347855 , acc: 0.875000\n",
      "Iteration 800: loss 0.210029 , acc: 0.937500\n",
      "Iteration 850: loss 0.520561 , acc: 0.812500\n",
      "Iteration 900: loss 0.299811 , acc: 0.875000\n",
      "Iteration 950: loss 0.257366 , acc: 0.875000\n",
      "Iteration 1000: loss 0.271144 , acc: 0.875000\n",
      "saved to ./weights_models/pretrained_lstm_SA.ckpt-1000\n",
      "Iteration 1050: loss 0.267094 , acc: 0.906250\n",
      "Iteration 1100: loss 0.340902 , acc: 0.906250\n",
      "Iteration 1150: loss 0.113465 , acc: 0.937500\n",
      "Iteration 1200: loss 0.215515 , acc: 0.906250\n",
      "Iteration 1250: loss 0.284507 , acc: 0.906250\n",
      "Iteration 1300: loss 0.222388 , acc: 0.875000\n",
      "Iteration 1350: loss 0.241055 , acc: 0.937500\n",
      "Iteration 1400: loss 0.259726 , acc: 0.875000\n",
      "Iteration 1450: loss 0.273091 , acc: 0.906250\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "writer = tf.summary.FileWriter(logdir, graph = sess.graph)\n",
    "\n",
    "istate = sess.run(zero_state)\n",
    "\n",
    "for i in range(iterations):\n",
    "    #next batch\n",
    "    input_batch, label_batch = next_batch(batch_size, input_train, y_train_mc)\n",
    "    #print(\"iterations\")\n",
    "    #print(i)\n",
    "    feed_dict = {input_data:input_batch, labels: label_batch}\n",
    "    for ke,va in enumerate(zero_state):\n",
    "        feed_dict[va] = istate[ke]\n",
    "        \n",
    "    _, ostate, l, acc = sess.run([optimizer, zero_state, loss, accuracy], \n",
    "                        feed_dict = feed_dict)\n",
    "    \n",
    "    # write summary to Tensorboard\n",
    "    if (i%50 == 0):        \n",
    "        print(\"Iteration %d: loss %f , acc: %f\"%(i, l, acc))\n",
    "        summary = sess.run(merged, feed_dict=feed_dict)\n",
    "        writer.add_summary(summary, i)\n",
    "    \n",
    "    if(i % 500 == 0 and i != 0):\n",
    "        save_path = saver.save(sess, \n",
    "                               \"./weights_models/pretrained_lstm_SA.ckpt\",\n",
    "                                global_step = i)\n",
    "        print(\"saved to %s\" % save_path)\n",
    "        \n",
    "    istate = ostate\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "* https://www.oreilly.com/learning/perform-sentiment-analysis-with-lstms-using-tensorflow\n",
    "* https://web.stanford.edu/class/cs20si/2017/lectures/notes_04.pdf\n",
    "* https://www.samyzaf.com/ML/imdb/imdb.html\n",
    "* https://keras.io/datasets/\n",
    "* https://stackoverflow.com/questions/48372994/multirnn-and-static-rnn-error-dimensions-must-be-equal-but-are-256-and-129?rq=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml.tools)",
   "language": "python",
   "name": "ml.tools"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
