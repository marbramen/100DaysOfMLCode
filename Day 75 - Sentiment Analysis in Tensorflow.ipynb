{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using cuDNN version 6021 on context None\n",
      "Mapped name None to device cuda: GeForce GTX 1050 (0000:01:00.0)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "maxlen = 250\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "input_train shape: (25000, 250)\n",
      "input_test shape: (25000, 250)\n",
      "y_train (25000,)\n",
      "y_test (25000,)\n"
     ]
    }
   ],
   "source": [
    "print(len(input_train), 'train sequences')\n",
    "print(len(input_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "input_train = sequence.pad_sequences(input_train, maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(input_test, maxlen=maxlen)\n",
    "\n",
    "print('input_train shape:', input_train.shape)\n",
    "print('input_test shape:', input_test.shape)\n",
    "print('y_train', y_train.shape)\n",
    "print('y_test', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_FROM = 3\n",
    "word_to_id = imdb.get_word_index()\n",
    "word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN \n",
      "Integers:\n",
      "----------\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    1    4  204 7610   20   16   93   11 9075   19    2\n",
      " 4390    6   55   52   22  849 4227  119    7 5259  961  178    6 1018\n",
      "  221   20 1184    2    2   29    7  265   16  530   17   29  220  210\n",
      "  468    8   30   11   32    7   27  102 5910 3634   17 3278 1881   16\n",
      "    6 6647    7 1262  190    4   20  122 2353    8   79    6  117  196\n",
      "   11 1370   12  127   24  847   33    4 1062    7    4 9075  310  131\n",
      "   12    9    6  253   20   15  144   30  110   33  222  280]\n",
      "\n",
      "\n",
      "Sentences:\n",
      "----------\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <START> the original demille movie was made in 1938 with <UNK> march a very good film indeed hollywood's love of remakes brings us a fairly interesting movie starring <UNK> <UNK> he of course was brilliant as he almost always seemed to be in all of his movies charlton heston as andrew jackson was a stroke of genius however the movie did tend to get a little long in places it does not move at the pace of the 1938 version still it is a fun movie that should be seen at least once\n",
      "\n",
      "\n",
      "TEST \n",
      "Sentence train : 24  value 1\n"
     ]
    }
   ],
   "source": [
    "id_example = 24\n",
    "print(\"TRAIN \")\n",
    "print(\"Integers:\")\n",
    "print(\"-\"*10)\n",
    "print(input_train[id_example])\n",
    "print(\"\\n\")\n",
    "print(\"Sentences:\")\n",
    "print(\"-\"*10)\n",
    "print(' '.join(id_to_word[id] for id in input_train[id_example]))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"TEST \")\n",
    "print(\"Sentence train :\", id_example, \" value\", y_train[id_example])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN \n",
      "Integers:\n",
      "----------\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    1  194 1153  194 8255   78  228    5    6\n",
      " 1463 4369 5012  134   26    4  715    8  118 1634   14  394   20   13\n",
      "  119  954  189  102    5  207  110 3103   21   14   69  188    8   30\n",
      "   23    7    4  249  126   93    4  114    9 2300 1523    5  647    4\n",
      "  116    9   35 8163    4  229    9  340 1322    4  118    9    4  130\n",
      " 4901   19    4 1002    5   89   29  952   46   37    4  455    9   45\n",
      "   43   38 1543 1905  398    4 1649   26 6853    5  163   11 3215    2\n",
      "    4 1153    9  194  775    7 8255    2  349 2637  148  605    2 8003\n",
      "   15  123  125   68    2 6853   15  349  165 4362   98    5    4  228\n",
      "    9   43    2 1157   15  299  120    5  120  174   11  220  175  136\n",
      "   50    9 4373  228 8255    5    2  656  245 2350    5    4 9837  131\n",
      "  152  491   18    2   32 7464 1212   14    9    6  371   78   22  625\n",
      "   64 1382    9    8  168  145   23    4 1690   15   16    4 1355    5\n",
      "   28    6   52  154  462   33   89   78  285   16  145   95]\n",
      "\n",
      "\n",
      "Sentences:\n",
      "----------\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <START> big hair big boobs bad music and a giant safety pin these are the words to best describe this terrible movie i love cheesy horror movies and i've seen hundreds but this had got to be on of the worst ever made the plot is paper thin and ridiculous the acting is an abomination the script is completely laughable the best is the end showdown with the cop and how he worked out who the killer is it's just so damn terribly written the clothes are sickening and funny in equal <UNK> the hair is big lots of boobs <UNK> men wear those cut <UNK> shirts that show off their <UNK> sickening that men actually wore them and the music is just <UNK> trash that plays over and over again in almost every scene there is trashy music boobs and <UNK> taking away bodies and the gym still doesn't close for <UNK> all joking aside this is a truly bad film whose only charm is to look back on the disaster that was the 80's and have a good old laugh at how bad everything was back then\n",
      "\n",
      "\n",
      "TEST \n",
      "Sentence train : 1  value 0\n"
     ]
    }
   ],
   "source": [
    "id_example = 1\n",
    "print(\"TRAIN \")\n",
    "print(\"Integers:\")\n",
    "print(\"-\"*10)\n",
    "print(input_train[id_example])\n",
    "print(\"\\n\")\n",
    "print(\"Sentences:\")\n",
    "print(\"-\"*10)\n",
    "print(' '.join(id_to_word[id] for id in input_train[id_example]))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"TEST \")\n",
    "print(\"Sentence train :\", id_example, \" value\", y_train[id_example])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train))\n",
    "print(y_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n",
      "(25000, 2)\n"
     ]
    }
   ],
   "source": [
    "#Convert y_train, y_test a multiclass\n",
    "def convert_to_multiclass(data):\n",
    "    y = []\n",
    "    for i in range(len(data)):\n",
    "        if data[i] == 1:\n",
    "            y.append([1,0])\n",
    "        else:\n",
    "            y.append([0,1])\n",
    "    y = np.array(y)\n",
    "    return y\n",
    "\n",
    "y_train_mc = convert_to_multiclass(y_train)\n",
    "y_test_mc = convert_to_multiclass(y_test)\n",
    "            \n",
    "print(y_train_mc.shape)\n",
    "print(y_test_mc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 250)\n",
      "(32, 2)\n"
     ]
    }
   ],
   "source": [
    "def next_batch(batch_size, data, labels):\n",
    "    idx = np.arange(0, len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:batch_size]\n",
    "    data_shuffle = [data[i] for i in idx]\n",
    "    labels_shuffle = [labels[i] for i in idx]\n",
    "    \n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n",
    "\n",
    "input_train_batch, y_train_batch = next_batch(32, input_train, y_train_mc)\n",
    "print(input_train_batch.shape)\n",
    "print(y_train_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 32\n",
    "lstmUnits = 64\n",
    "numClasses = 2\n",
    "iterations = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholders for input and labels\n",
    "\n",
    "tf.reset_default_graph()\n",
    "labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, maxlen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding layer\n",
    "num_embeddings = 300\n",
    "\n",
    "embedding_matrix = tf.Variable(tf.random_uniform([max_features, num_embeddings], \n",
    "                                                 -1.0, \n",
    "                                                 1.0))\n",
    "\n",
    "embedding_layer = tf.nn.embedding_lookup(embedding_matrix, input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 250, 300)\n"
     ]
    }
   ],
   "source": [
    "# LSTM layer and dropout layer\n",
    "print(embedding_layer.get_shape())\n",
    "\n",
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob = 0.75)\n",
    "encode_lstm, _ = tf.nn.dynamic_rnn(lstmCell, embedding_layer, dtype=tf.float32)\n",
    "\n",
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "value = tf.transpose(encode_lstm, [1,0,2])\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "prediction = (tf.matmul(last, weight) + bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics for correct prediction and accuracy\n",
    "correctPred = tf.equal(tf.argmax(prediction, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard cross entropy loss with softmax layer\n",
    "# adam optmizer\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, \n",
    "                                                              labels = labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"/home/marchelo/MarcheloBragagnini/100DaysOfMLCode/tensorboard\" + \"/\" + datetime.datetime.now().strftime(\"%Y%m%s-%H%M%S\") + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: loss 0.866857 , acc: 0.656250\n",
      "Iteration 50: loss 0.787355 , acc: 0.593750\n",
      "Iteration 100: loss 0.577016 , acc: 0.656250\n",
      "Iteration 150: loss 0.689408 , acc: 0.718750\n",
      "Iteration 200: loss 0.628536 , acc: 0.687500\n",
      "Iteration 250: loss 0.898785 , acc: 0.500000\n",
      "Iteration 300: loss 0.473064 , acc: 0.718750\n",
      "Iteration 350: loss 0.460054 , acc: 0.781250\n",
      "Iteration 400: loss 0.506524 , acc: 0.781250\n",
      "Iteration 450: loss 0.656836 , acc: 0.687500\n",
      "Iteration 500: loss 0.386893 , acc: 0.906250\n",
      "Iteration 550: loss 0.474001 , acc: 0.750000\n",
      "Iteration 600: loss 0.319392 , acc: 0.812500\n",
      "Iteration 650: loss 0.392945 , acc: 0.812500\n",
      "Iteration 700: loss 0.239292 , acc: 0.875000\n",
      "Iteration 750: loss 0.537063 , acc: 0.781250\n",
      "Iteration 800: loss 0.399117 , acc: 0.875000\n",
      "Iteration 850: loss 0.382770 , acc: 0.781250\n",
      "Iteration 900: loss 0.283908 , acc: 0.906250\n",
      "Iteration 950: loss 0.566400 , acc: 0.781250\n",
      "Iteration 1000: loss 0.421888 , acc: 0.750000\n",
      "Iteration 1050: loss 0.306210 , acc: 0.906250\n",
      "Iteration 1100: loss 0.273011 , acc: 0.906250\n",
      "Iteration 1150: loss 0.080973 , acc: 0.968750\n",
      "Iteration 1200: loss 0.280155 , acc: 0.906250\n",
      "Iteration 1250: loss 0.318859 , acc: 0.875000\n",
      "Iteration 1300: loss 0.306210 , acc: 0.843750\n",
      "Iteration 1350: loss 0.202026 , acc: 0.937500\n",
      "Iteration 1400: loss 0.391572 , acc: 0.812500\n",
      "Iteration 1450: loss 0.304598 , acc: 0.812500\n",
      "Iteration 1500: loss 0.169677 , acc: 0.968750\n",
      "Iteration 1550: loss 0.228129 , acc: 0.937500\n",
      "Iteration 1600: loss 0.245214 , acc: 0.906250\n",
      "Iteration 1650: loss 0.091474 , acc: 0.968750\n",
      "Iteration 1700: loss 0.105745 , acc: 0.968750\n",
      "Iteration 1750: loss 0.179476 , acc: 0.968750\n",
      "Iteration 1800: loss 0.302144 , acc: 0.906250\n",
      "Iteration 1850: loss 0.078816 , acc: 1.000000\n",
      "Iteration 1900: loss 0.128987 , acc: 0.968750\n",
      "Iteration 1950: loss 0.112075 , acc: 0.968750\n",
      "Iteration 2000: loss 0.116503 , acc: 0.968750\n",
      "Iteration 2050: loss 0.194235 , acc: 0.937500\n",
      "Iteration 2100: loss 0.036634 , acc: 1.000000\n",
      "Iteration 2150: loss 0.141068 , acc: 0.937500\n",
      "Iteration 2200: loss 0.188092 , acc: 0.906250\n",
      "Iteration 2250: loss 0.236831 , acc: 0.906250\n",
      "Iteration 2300: loss 0.077509 , acc: 0.968750\n",
      "Iteration 2350: loss 0.130773 , acc: 0.968750\n",
      "Iteration 2400: loss 0.314330 , acc: 0.968750\n",
      "Iteration 2450: loss 0.036370 , acc: 1.000000\n",
      "Iteration 2500: loss 0.133498 , acc: 0.968750\n",
      "Iteration 2550: loss 0.101114 , acc: 0.968750\n",
      "Iteration 2600: loss 0.039079 , acc: 1.000000\n",
      "Iteration 2650: loss 0.158497 , acc: 0.937500\n",
      "Iteration 2700: loss 0.260338 , acc: 0.906250\n",
      "Iteration 2750: loss 0.066860 , acc: 0.968750\n",
      "Iteration 2800: loss 0.155170 , acc: 0.906250\n",
      "Iteration 2850: loss 0.066304 , acc: 0.968750\n",
      "Iteration 2900: loss 0.080681 , acc: 0.968750\n",
      "Iteration 2950: loss 0.221361 , acc: 0.968750\n",
      "Iteration 3000: loss 0.142740 , acc: 0.968750\n",
      "Iteration 3050: loss 0.170962 , acc: 0.875000\n",
      "Iteration 3100: loss 0.211253 , acc: 0.906250\n",
      "Iteration 3150: loss 0.203665 , acc: 0.875000\n",
      "Iteration 3200: loss 0.111900 , acc: 0.968750\n",
      "Iteration 3250: loss 0.011590 , acc: 1.000000\n",
      "Iteration 3300: loss 0.050438 , acc: 1.000000\n",
      "Iteration 3350: loss 0.119689 , acc: 0.968750\n",
      "Iteration 3400: loss 0.106569 , acc: 0.968750\n",
      "Iteration 3450: loss 0.199338 , acc: 0.937500\n",
      "Iteration 3500: loss 0.046706 , acc: 1.000000\n",
      "Iteration 3550: loss 0.242116 , acc: 0.906250\n",
      "Iteration 3600: loss 0.076580 , acc: 0.968750\n",
      "Iteration 3650: loss 0.041369 , acc: 1.000000\n",
      "Iteration 3700: loss 0.214838 , acc: 0.968750\n",
      "Iteration 3750: loss 0.145693 , acc: 0.906250\n",
      "Iteration 3800: loss 0.068974 , acc: 0.968750\n",
      "Iteration 3850: loss 0.042537 , acc: 0.968750\n",
      "Iteration 3900: loss 0.080887 , acc: 0.968750\n",
      "Iteration 3950: loss 0.019783 , acc: 1.000000\n",
      "Iteration 4000: loss 0.054847 , acc: 0.968750\n",
      "Iteration 4050: loss 0.107020 , acc: 0.937500\n",
      "Iteration 4100: loss 0.033001 , acc: 1.000000\n",
      "Iteration 4150: loss 0.017705 , acc: 1.000000\n",
      "Iteration 4200: loss 0.015966 , acc: 1.000000\n",
      "Iteration 4250: loss 0.012051 , acc: 1.000000\n",
      "Iteration 4300: loss 0.007765 , acc: 1.000000\n",
      "Iteration 4350: loss 0.024048 , acc: 1.000000\n",
      "Iteration 4400: loss 0.135492 , acc: 0.968750\n",
      "Iteration 4450: loss 0.018046 , acc: 1.000000\n",
      "Iteration 4500: loss 0.076706 , acc: 0.968750\n",
      "Iteration 4550: loss 0.018435 , acc: 1.000000\n",
      "Iteration 4600: loss 0.236027 , acc: 0.937500\n",
      "Iteration 4650: loss 0.025614 , acc: 1.000000\n",
      "Iteration 4700: loss 0.224399 , acc: 0.937500\n",
      "Iteration 4750: loss 0.025011 , acc: 1.000000\n",
      "Iteration 4800: loss 0.060931 , acc: 1.000000\n",
      "Iteration 4850: loss 0.085548 , acc: 0.968750\n",
      "Iteration 4900: loss 0.010990 , acc: 1.000000\n",
      "Iteration 4950: loss 0.011638 , acc: 1.000000\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "writer = tf.summary.FileWriter(logdir, graph = sess.graph)\n",
    "\n",
    "for i in range(iterations):\n",
    "    #next batch\n",
    "    input_batch, label_batch = next_batch(batch_size, input_train, y_train_mc)\n",
    "    _, l, acc = sess.run([optimizer, loss, accuracy], {input_data:input_batch, labels: label_batch})\n",
    "    \n",
    "    # write summary to Tensorboard\n",
    "    if (i%50 == 0):\n",
    "        print(\"Iteration %d: loss %f , acc: %f\"%(i, l, acc))\n",
    "        summary = sess.run(merged, {input_data:input_batch, labels: label_batch})\n",
    "        writer.add_summary(summary, i)\n",
    "    \n",
    "    if(i % 10000 == 0 and i != 0):\n",
    "        save_path = saver.save(sess, \n",
    "                               \"./weights_models/pretrained_lstm_SA.ckpt\",\n",
    "                                global_step = i)\n",
    "        print(\"saved to %s\" % save_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "* https://www.oreilly.com/learning/perform-sentiment-analysis-with-lstms-using-tensorflow\n",
    "* https://web.stanford.edu/class/cs20si/2017/lectures/notes_04.pdf\n",
    "* https://www.samyzaf.com/ML/imdb/imdb.html\n",
    "* https://keras.io/datasets/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml.tools)",
   "language": "python",
   "name": "ml.tools"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
