{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Neural Network\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/day63_01.png\" width=\"400\">\n",
    "        $$\\mathbf{h} = \\text{sigmoid}(\\mathbf{xW_1}+\\mathbf{b_1})$$\n",
    "    $$\\mathbf{\\hat{y}} = \\text{softmax}(\\mathbf{hW_2}+\\mathbf{b_2})$$\n",
    "\n",
    "<br>\n",
    "Figure 1. Image from CS224n NLP with Deep Learning\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    #matrix\n",
    "    if len(x.shape) > 1:\n",
    "        c = -1*x.max(axis=1, keepdims=True)\n",
    "        x = x + c\n",
    "        x_exp = np.exp(x)\n",
    "        x = x_exp/np.sum(x_exp, axis=1, keepdims=True)\n",
    "    #vector\n",
    "    else:\n",
    "        c = -1*x.max()\n",
    "        x = x + c\n",
    "        x_exp = np.exp(x)\n",
    "        x = x_exp/np.sum(x_exp)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def grad_sigmoid(s):\n",
    "    # s = sigmoid(x)\n",
    "    return s*(1-s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward_pass(X, labels, params, dimensions):\n",
    "    ofs = 0\n",
    "    Dx, H, Dy = (dimensions[0], dimensions[1], dimensions[2])\n",
    "    \n",
    "    W1 = np.reshape(params[ofs:ofs + Dx*H], (Dx,H))\n",
    "    ofs += Dx * H\n",
    "    b1 = np.reshape(params[ofs:ofs + H], (1,H))\n",
    "    ofs += H\n",
    "    W2 = np.reshape(params[ofs:ofs + H*Dy], (H,Dy))\n",
    "    ofs += H*Dy\n",
    "    b2 = np.reshape(params[ofs:ofs + Dy], (1,Dy))\n",
    "    \n",
    "    # Forward pass\n",
    "    z1 = np.dot(X,W1) + b1\n",
    "    h = sigmoid(z1)\n",
    "    z2 = np.dot(h,W2) + b2\n",
    "    y = softmax(z2)\n",
    "    \n",
    "    cost = -1*np.sum(labels*np.log(y))\n",
    "    \n",
    "    #Backward pass\n",
    "    gradz2 = y - labels\n",
    "    gradW2 = np.dot(h.T, gradz2)\n",
    "    gradb2 = gradz2\n",
    "    gradW1 = np.dot(X.T, grad_sigmoid(h) * np.dot(gradz2, W2.T))\n",
    "    gradb1 = np.sum(grad_sigmoid(h) * np.dot(gradz2, W2.T), axis=0, keepdims=True)\n",
    "    \n",
    "    grad = np.concatenate((gradW1.flatten(), gradb1.flatten(), \n",
    "                           gradW2.flatten(), gradb2.flatten()))\n",
    "    \n",
    "    return cost, grad, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_checking(f, x):\n",
    "    fx, grad, _ = f(x)\n",
    "    h = 1e-4\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index        \n",
    "        xtemp = x[ix]\n",
    "        \n",
    "        x[ix] = x[ix] + h\n",
    "        f_plus, _, _ = f(x)\n",
    "        x[ix] = xtemp\n",
    "        \n",
    "        x[ix] = x[ix] - h\n",
    "        f_minus, _, _ = f(x)\n",
    "        x[ix] = x_temp\n",
    "        \n",
    "        numgrad = (f_plus - f_minus)/(2*h)\n",
    "        \n",
    "        reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))\n",
    "        if reldiff > 1e-5:\n",
    "            print(\"Gradient check failed.\")\n",
    "            print(\"First gradient error found at index %s\" % str(ix))\n",
    "            print(\"Your gradient: %f \\t Numerical gradient: %f\" % (\n",
    "                grad[ix], numgrad))\n",
    "            return\n",
    "\n",
    "        it.iternext() # Step to next dimension\n",
    "    print(\"Gradient check passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(params, grad, learning_rate):\n",
    "    for ix in range(0, len(params)):\n",
    "        params[ix] = params[ix] - learning_rate * grad[ix]\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(labels, y):\n",
    "    m, _ = labels.shape\n",
    "    idx_labels = np.argmax(labels, axis=1)\n",
    "    idx_y = np.argmax(y, axis=1)    \n",
    "    count = np.sum(idx_labels == idx_y)\n",
    "    \n",
    "    return count / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 \t cost: 59.859510 \t accuracy: 0.100000\n",
      "Iteration 500 \t cost: 15.568468 \t accuracy: 0.750000\n",
      "Iteration 1000 \t cost: 9.455554 \t accuracy: 0.850000\n",
      "Iteration 1500 \t cost: 5.064316 \t accuracy: 0.950000\n",
      "Iteration 2000 \t cost: 3.248469 \t accuracy: 0.950000\n",
      "Iteration 2500 \t cost: 2.361378 \t accuracy: 1.000000\n",
      "Iteration 3000 \t cost: 1.596314 \t accuracy: 1.000000\n",
      "Iteration 3500 \t cost: 1.169276 \t accuracy: 1.000000\n",
      "Iteration 4000 \t cost: 0.925379 \t accuracy: 1.000000\n",
      "Iteration 4500 \t cost: 0.768001 \t accuracy: 1.000000\n",
      "Iteration 5000 \t cost: 0.657157 \t accuracy: 1.000000\n",
      "Iteration 5500 \t cost: 0.574506 \t accuracy: 1.000000\n",
      "Iteration 6000 \t cost: 0.510382 \t accuracy: 1.000000\n",
      "Iteration 6500 \t cost: 0.459141 \t accuracy: 1.000000\n",
      "Iteration 7000 \t cost: 0.417242 \t accuracy: 1.000000\n",
      "Iteration 7500 \t cost: 0.382340 \t accuracy: 1.000000\n",
      "Iteration 8000 \t cost: 0.352819 \t accuracy: 1.000000\n",
      "Iteration 8500 \t cost: 0.327524 \t accuracy: 1.000000\n",
      "Iteration 9000 \t cost: 0.305610 \t accuracy: 1.000000\n",
      "Iteration 9500 \t cost: 0.286442 \t accuracy: 1.000000\n",
      "----------\n",
      "final cost 0.269569\n",
      "accuracy 1.000000\n"
     ]
    }
   ],
   "source": [
    "N = 20\n",
    "dimensions = [10, 5, 10]\n",
    "data = np.random.randn(N, dimensions[0])   # each row will be a datum\n",
    "labels = np.zeros((N, dimensions[2]))\n",
    "for i in range(N):\n",
    "    labels[i, random.randint(0,dimensions[2]-1)] = 1\n",
    "\n",
    "params = np.random.randn((dimensions[0] + 1) * dimensions[1] + (\n",
    "        dimensions[1] + 1) * dimensions[2], )\n",
    "\n",
    "n_iterations = 10000\n",
    "learning_rate = 0.01\n",
    "\n",
    "for i in range(0, n_iterations):\n",
    "    cost, grad, y_pred = forward_backward_pass(data, labels, params, dimensions)\n",
    "    params = update_parameters(params, grad, learning_rate)\n",
    "    \n",
    "    if i % 500 == 0:\n",
    "        print(\"Iteration %d \\t cost: %f \\t accuracy: %f\"%(i, cost, accuracy(labels, y_pred)))\n",
    "print(\"-\"*10)\n",
    "print(\"final cost %f\"%(cost))\n",
    "print(\"accuracy %f\"%(accuracy(labels, y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml.tools)",
   "language": "python",
   "name": "ml.tools"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
